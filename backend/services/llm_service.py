"""
LLM Service for AI-powered question parsing and grading
"""
import os
import json
from typing import List, Dict, Any, Optional
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
import httpx

from models import QuestionType
from utils import calculate_content_hash


class LLMService:
    """Service for interacting with various LLM providers"""

    def __init__(self, config: Optional[Dict[str, str]] = None):
        """
        Initialize LLM Service with optional configuration.
        If config is not provided, falls back to environment variables.

        Args:
            config: Dictionary with keys like 'ai_provider', 'openai_api_key', etc.
        """
        # Get provider from config or environment
        self.provider = (config or {}).get("ai_provider") or os.getenv("AI_PROVIDER", "openai")

        if self.provider == "openai":
            api_key = (config or {}).get("openai_api_key") or os.getenv("OPENAI_API_KEY")
            base_url = (config or {}).get("openai_base_url") or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
            self.model = (config or {}).get("openai_model") or os.getenv("OPENAI_MODEL", "gpt-4o-mini")

            if not api_key:
                raise ValueError("OpenAI API key not configured")

            self.client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=120.0,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’
                max_retries=3   # è‡ªåŠ¨é‡è¯• 3 æ¬¡
            )

            # Log configuration for debugging
            print(f"[LLM Config] Provider: OpenAI", flush=True)
            print(f"[LLM Config] Base URL: {base_url}", flush=True)
            print(f"[LLM Config] Model: {self.model}", flush=True)
            print(f"[LLM Config] API Key: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else 'xxxx'}", flush=True)

        elif self.provider == "anthropic":
            api_key = (config or {}).get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY")
            self.model = (config or {}).get("anthropic_model") or os.getenv("ANTHROPIC_MODEL", "claude-3-haiku-20240307")

            if not api_key:
                raise ValueError("Anthropic API key not configured")

            self.client = AsyncAnthropic(
                api_key=api_key
            )

        elif self.provider == "qwen":
            api_key = (config or {}).get("qwen_api_key") or os.getenv("QWEN_API_KEY")
            base_url = (config or {}).get("qwen_base_url") or os.getenv("QWEN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
            self.model = (config or {}).get("qwen_model") or os.getenv("QWEN_MODEL", "qwen-plus")

            if not api_key:
                raise ValueError("Qwen API key not configured")

            self.client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=120.0,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’
                max_retries=3   # è‡ªåŠ¨é‡è¯• 3 æ¬¡
            )

        elif self.provider == "gemini":
            api_key = (config or {}).get("gemini_api_key") or os.getenv("GEMINI_API_KEY")
            base_url = (config or {}).get("gemini_base_url") or os.getenv("GEMINI_BASE_URL")
            self.model = (config or {}).get("gemini_model") or os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp")

            if not api_key:
                raise ValueError("Gemini API key not configured")

            # Store Gemini configuration for REST API calls
            self.gemini_api_key = api_key
            self.gemini_base_url = base_url or "https://generativelanguage.googleapis.com"

            # Create httpx client for REST API calls (instead of SDK)
            self.client = httpx.AsyncClient(
                timeout=120.0,
                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
            )

            # Log configuration for debugging
            print(f"[LLM Config] Provider: Gemini (REST API)", flush=True)
            print(f"[LLM Config] Model: {self.model}", flush=True)
            print(f"[LLM Config] Base URL: {self.gemini_base_url}", flush=True)
            print(f"[LLM Config] API Key: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else 'xxxx'}", flush=True)

        else:
            raise ValueError(f"Unsupported AI provider: {self.provider}")

    async def parse_document(self, content: str) -> List[Dict[str, Any]]:
        """
        Parse document content and extract questions.

        Returns a list of dictionaries with question data:
        [
            {
                "content": "Question text",
                "type": "single/multiple/judge/short",
                "options": ["A. Option1", "B. Option2", ...],  # For choice questions
                "answer": "Correct answer",
                "analysis": "Explanation"
            },
            ...
        ]
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯•é¢˜è§£æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä¸‹é¢çš„æ–‡æ¡£å†…å®¹ï¼Œæå–å…¶ä¸­çš„æ‰€æœ‰è¯•é¢˜ã€‚

**è¯†åˆ«è§„åˆ™**ï¼š
- æ–‡æ¡£ä¸­å¯èƒ½åŒ…å«ä¸­æ–‡æˆ–è‹±æ–‡é¢˜ç›®
- é¢˜ç›®å¯èƒ½æœ‰å¤šç§æ ¼å¼ï¼Œè¯·çµæ´»è¯†åˆ«
- **é‡è¦**ï¼šåªæå–å®Œæ•´çš„é¢˜ç›®ï¼Œå¿½ç•¥ä»»ä½•ä¸å®Œæ•´çš„é¢˜ç›®ï¼ˆé¢˜ç›®è¢«æˆªæ–­ã€ç¼ºå°‘é€‰é¡¹ã€ç¼ºå°‘å…³é”®ä¿¡æ¯ç­‰ï¼‰
- å¦‚æœé¢˜ç›®çœ‹èµ·æ¥ä¸å®Œæ•´ï¼ˆæ¯”å¦‚å¼€å¤´æˆ–ç»“å°¾è¢«åˆ‡æ–­ï¼‰ï¼Œç›´æ¥è·³è¿‡è¯¥é¢˜ç›®
- å¦‚æœæ–‡æ¡£åªæ˜¯æ™®é€šæ–‡ç« è€Œæ²¡æœ‰é¢˜ç›®ï¼Œè¯·è¿”å›ç©ºæ•°ç»„ []

**é¢˜ç›®ç±»å‹è¯†åˆ«** (ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹4ç§ç±»å‹ä¹‹ä¸€)ï¼š
1. **single** - å•é€‰é¢˜ï¼šåªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆçš„é€‰æ‹©é¢˜
2. **multiple** - å¤šé€‰é¢˜ï¼šæœ‰å¤šä¸ªæ­£ç¡®ç­”æ¡ˆçš„é€‰æ‹©é¢˜ï¼ˆç­”æ¡ˆæ ¼å¼å¦‚ï¼šABã€ABCã€ACDç­‰ï¼‰
3. **judge** - åˆ¤æ–­é¢˜ï¼šå¯¹é”™/æ˜¯é/True Falseé¢˜ç›®
4. **short** - ç®€ç­”é¢˜ï¼šåŒ…æ‹¬é—®ç­”ã€è®¡ç®—ã€è¯æ˜ã€å¡«ç©ºã€ç¼–ç¨‹ç­‰æ‰€æœ‰éé€‰æ‹©é¢˜

**å¤šé€‰é¢˜è¯†åˆ«å…³é”®è¯**ï¼š
- æ˜ç¡®æ ‡æ³¨"å¤šé€‰"ã€"å¤šé¡¹é€‰æ‹©"ã€"Multiple Choice"
- é¢˜å¹²ä¸­åŒ…å«"å¯èƒ½"ã€"æ­£ç¡®çš„æœ‰"ã€"åŒ…æ‹¬"ç­‰
- ç­”æ¡ˆæ˜¯å¤šä¸ªå­—æ¯ç»„åˆï¼ˆå¦‚ï¼šABCã€BDã€ABCDï¼‰

**æ¯é“é¢˜ç›®æå–å­—æ®µ**ï¼š
1. **content**: å®Œæ•´çš„é¢˜ç›®æ–‡å­—ï¼ˆå»é™¤é¢˜å·ï¼‰
2. **type**: é¢˜ç›®ç±»å‹ï¼ˆsingle/multiple/judge/shortï¼‰
3. **options**: é€‰é¡¹æ•°ç»„ï¼ˆä»…é€‰æ‹©é¢˜ï¼Œæ ¼å¼: ["A. é€‰é¡¹1", "B. é€‰é¡¹2", ...]ï¼‰
4. **answer**: æ­£ç¡®ç­”æ¡ˆ
   - å•é€‰é¢˜: å•ä¸ªå­—æ¯ (å¦‚ "A"ã€"B")
   - å¤šé€‰é¢˜: å¤šä¸ªå­—æ¯æ— ç©ºæ ¼ (å¦‚ "AB"ã€"ABC"ã€"BD")
   - åˆ¤æ–­é¢˜: "å¯¹"/"é”™"ã€"æ­£ç¡®"/"é”™è¯¯"ã€"True"/"False"
   - ç®€ç­”é¢˜: å®Œæ•´ç­”æ¡ˆæ–‡æœ¬ï¼Œå¦‚æœæ²¡æœ‰ç­”æ¡ˆå¡« null
5. **analysis**: è§£æè¯´æ˜ï¼ˆå¦‚æœæœ‰ï¼‰

**JSON æ ¼å¼è¦æ±‚**ï¼š
- å¿…é¡»è¿”å›ä¸€ä¸ªå®Œæ•´çš„ JSON æ•°ç»„ (ä»¥ [ å¼€å§‹ï¼Œä»¥ ] ç»“æŸ)
- ä¸è¦åŒ…å« markdown ä»£ç å—æ ‡è®° (```json æˆ– ```)
- ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—
- å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦å¿…é¡»æ­£ç¡®è½¬ä¹‰ï¼ˆæ¢è¡Œç”¨ \\nï¼Œå¼•å·ç”¨ \\"ï¼Œåæ–œæ ç”¨ \\\\ï¼‰
- ä¸è¦åœ¨å­—ç¬¦ä¸²å€¼ä¸­ä½¿ç”¨æœªè½¬ä¹‰çš„æ§åˆ¶å­—ç¬¦

**è¿”å›æ ¼å¼ç¤ºä¾‹**ï¼š
[
  {{
    "content": "ä¸‹åˆ—å…³äºPythonçš„æè¿°ï¼Œæ­£ç¡®çš„æ˜¯",
    "type": "single",
    "options": ["A. Pythonæ˜¯ç¼–è¯‘å‹è¯­è¨€", "B. Pythonæ”¯æŒé¢å‘å¯¹è±¡ç¼–ç¨‹", "C. Pythonä¸æ”¯æŒå‡½æ•°å¼ç¼–ç¨‹", "D. Pythonåªèƒ½ç”¨äºWebå¼€å‘"],
    "answer": "B",
    "analysis": "Pythonæ˜¯è§£é‡Šå‹è¯­è¨€ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼"
  }},
  {{
    "content": "ä»¥ä¸‹å“ªäº›æ˜¯Pythonçš„ä¼˜ç‚¹ï¼ˆå¤šé€‰ï¼‰",
    "type": "multiple",
    "options": ["A. è¯­æ³•ç®€æ´", "B. åº“ä¸°å¯Œ", "C. æ‰§è¡Œé€Ÿåº¦æœ€å¿«", "D. æ˜“äºå­¦ä¹ "],
    "answer": "ABD",
    "analysis": "Pythonä¼˜ç‚¹æ˜¯è¯­æ³•ç®€æ´ã€åº“ä¸°å¯Œã€æ˜“å­¦ï¼Œä½†æ‰§è¡Œé€Ÿåº¦ä¸æ˜¯æœ€å¿«çš„"
  }},
  {{
    "content": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
    "type": "judge",
    "options": [],
    "answer": "å¯¹",
    "analysis": null
  }},
  {{
    "content": "è¯·è§£é‡ŠPythonä¸­çš„è£…é¥°å™¨æ˜¯ä»€ä¹ˆ",
    "type": "short",
    "options": [],
    "answer": "è£…é¥°å™¨æ˜¯Pythonä¸­ä¸€ç§ç‰¹æ®Šçš„å‡½æ•°ï¼Œç”¨äºä¿®æ”¹å…¶ä»–å‡½æ•°çš„è¡Œä¸º...",
    "analysis": null
  }}
]

**æ–‡æ¡£å†…å®¹**ï¼š
---
{content}
---

**æœ€åæé†’**ï¼š
- ä»”ç»†è¯†åˆ«å¤šé€‰é¢˜ï¼ˆçœ‹é¢˜å¹²ã€çœ‹ç­”æ¡ˆæ ¼å¼ï¼‰
- å•é€‰å’Œå¤šé€‰å®¹æ˜“æ··æ·†ï¼Œè¯·ç‰¹åˆ«æ³¨æ„åŒºåˆ†
- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰é¢˜ç›®ï¼Œè¿”å› []
- åªè¿”å› JSON æ•°ç»„ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹"""

        try:
            if self.provider == "anthropic":
                response = await self.client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    messages=[
                        {"role": "user", "content": prompt.format(content=content)}
                    ]
                )
                result = response.content[0].text
            elif self.provider == "gemini":
                # Gemini uses REST API
                print(f"[Gemini Text] Calling Gemini REST API with model: {self.model}", flush=True)

                url = f"{self.gemini_base_url}/v1beta/models/{self.model}:generateContent"
                headers = {"Content-Type": "application/json"}
                params = {"key": self.gemini_api_key}
                payload = {
                    "contents": [{
                        "parts": [{"text": prompt.format(content=content)}]
                    }]
                }

                response = await self.client.post(url, headers=headers, params=params, json=payload)
                response.raise_for_status()
                response_data = response.json()

                # Extract text from response
                result = response_data["candidates"][0]["content"]["parts"][0]["text"]
                print(f"[Gemini Text] API call completed", flush=True)
            else:  # OpenAI or Qwen
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a professional question parser. Return only JSON."},
                        {"role": "user", "content": prompt.format(content=content)}
                    ],
                    temperature=0.3,
                )
                result = response.choices[0].message.content

            # Log original response for debugging
            import sys
            print(f"[LLM Raw Response] Length: {len(result)} chars", flush=True)
            print(f"[LLM Raw Response] First 300 chars:\n{result[:300]}", flush=True)
            print(f"[LLM Raw Response] Last 200 chars:\n{result[-200:]}", flush=True)
            sys.stdout.flush()

            # Clean result and parse JSON
            result = result.strip()

            # Remove markdown code blocks
            if result.startswith("```json"):
                result = result[7:]
            elif result.startswith("```"):
                result = result[3:]

            if result.endswith("```"):
                result = result[:-3]

            result = result.strip()

            # Try to find JSON array if there's extra text
            if not result.startswith('['):
                # Find the first '[' character
                start_idx = result.find('[')
                if start_idx != -1:
                    print(f"[JSON Cleanup] Found '[' at position {start_idx}, extracting array...")
                    result = result[start_idx:]
                else:
                    print(f"[JSON Error] No '[' found in response!")
                    raise Exception("LLM response does not contain a JSON array")

            if not result.endswith(']'):
                # Find the last ']' character
                end_idx = result.rfind(']')
                if end_idx != -1:
                    print(f"[JSON Cleanup] Found last ']' at position {end_idx}")
                    result = result[:end_idx + 1]

            result = result.strip()

            # Additional cleanup: fix common JSON issues
            # 1. Remove trailing commas before closing brackets
            import re
            result = re.sub(r',(\s*[}\]])', r'\1', result)

            # 2. Fix unescaped quotes in string values (basic attempt)
            # This is tricky and may not catch all cases, but helps with common issues

            # Log the cleaned result for debugging
            print(f"[LLM Cleaned JSON] Length: {len(result)} chars")
            print(f"[LLM Cleaned JSON] First 300 chars:\n{result[:300]}")

            try:
                questions = json.loads(result)
            except json.JSONDecodeError as je:
                print(f"[JSON Error] Failed to parse JSON at line {je.lineno}, column {je.colno}")
                print(f"[JSON Error] Error: {je.msg}")

                # If error is about control characters, try to fix them
                if "control character" in je.msg.lower() or "invalid \\escape" in je.msg.lower():
                    print(f"[JSON Cleanup] Attempting to fix control characters...", flush=True)

                    # Fix unescaped control characters in JSON string values
                    import re

                    def fix_string_value(match):
                        """Fix control characters inside a JSON string value"""
                        string_content = match.group(1)
                        # Escape control characters
                        string_content = string_content.replace('\n', '\\n')
                        string_content = string_content.replace('\r', '\\r')
                        string_content = string_content.replace('\t', '\\t')
                        string_content = string_content.replace('\b', '\\b')
                        string_content = string_content.replace('\f', '\\f')
                        return f'"{string_content}"'

                    # Match string values in JSON
                    # Pattern matches: "..." (handles escaped quotes and backslashes)
                    # (?:[^"\\]|\\.)* means: either non-quote-non-backslash OR backslash-followed-by-anything, repeated
                    fixed_result = re.sub(r'"((?:[^"\\]|\\.)*)"', fix_string_value, result)

                    print(f"[JSON Cleanup] Retrying with fixed control characters...", flush=True)
                    try:
                        questions = json.loads(fixed_result)
                        print(f"[JSON Cleanup] âœ… Successfully parsed after fixing control characters!", flush=True)
                    except json.JSONDecodeError as je2:
                        print(f"[JSON Error] Still failed after fix: {je2.msg}", flush=True)
                        # Print context around the error
                        lines = result.split('\n')
                        if je.lineno <= len(lines):
                            start = max(0, je.lineno - 3)
                            end = min(len(lines), je.lineno + 2)
                            print(f"[JSON Error] Context (lines {start+1}-{end}):")
                            for i in range(start, end):
                                marker = " >>> " if i == je.lineno - 1 else "     "
                                print(f"{marker}{i+1}: {lines[i]}")
                        raise Exception(f"Invalid JSON format from LLM: {je.msg} at line {je.lineno}")
                else:
                    # Print context around the error
                    lines = result.split('\n')
                    if je.lineno <= len(lines):
                        start = max(0, je.lineno - 3)
                        end = min(len(lines), je.lineno + 2)
                        print(f"[JSON Error] Context (lines {start+1}-{end}):")
                        for i in range(start, end):
                            marker = " >>> " if i == je.lineno - 1 else "     "
                            print(f"{marker}{i+1}: {lines[i]}")
                    raise Exception(f"Invalid JSON format from LLM: {je.msg} at line {je.lineno}")

            # Validate that we got a list
            if not isinstance(questions, list):
                raise Exception(f"Expected a list of questions, got {type(questions)}")

            if len(questions) == 0:
                raise Exception("No questions found in the parsed result")

            # Validate and fix question types
            valid_types = {"single", "multiple", "judge", "short"}
            type_mapping = {
                "proof": "short",
                "essay": "short",
                "calculation": "short",
                "fill": "short",
                "å¡«ç©º": "short",
                "è¯æ˜": "short",
                "è®¡ç®—": "short",
                "é—®ç­”": "short",
                "å•é€‰": "single",
                "å¤šé€‰": "multiple",
                "åˆ¤æ–­": "judge",
                "ç®€ç­”": "short"
            }

            # Add content hash and validate types
            for q in questions:
                if "content" not in q:
                    print(f"[Warning] Question missing 'content' field: {q}")
                    continue

                # Validate and fix question type
                q_type = q.get("type", "short")
                if isinstance(q_type, str):
                    q_type_lower = q_type.lower()
                    if q_type_lower not in valid_types:
                        # Try to map to valid type
                        if q_type_lower in type_mapping:
                            old_type = q_type
                            q["type"] = type_mapping[q_type_lower]
                            print(f"[Type Fix] Changed '{old_type}' to '{q['type']}' for question: {q['content'][:50]}...", flush=True)
                        else:
                            # Default to short answer
                            print(f"[Type Fix] Unknown type '{q_type}', defaulting to 'short' for question: {q['content'][:50]}...", flush=True)
                            q["type"] = "short"
                    else:
                        q["type"] = q_type_lower
                else:
                    q["type"] = "short"

                q["content_hash"] = calculate_content_hash(q["content"])

            return questions

        except Exception as e:
            print(f"[Error] Document parsing failed: {str(e)}")
            raise Exception(f"Failed to parse document: {str(e)}")

    def split_pdf_pages(self, pdf_bytes: bytes, pages_per_chunk: int = 4, overlap: int = 1) -> List[bytes]:
        """
        Split PDF into overlapping chunks to handle long documents.

        Args:
            pdf_bytes: PDF file content
            pages_per_chunk: Number of pages per chunk (default: 4)
            overlap: Number of overlapping pages between chunks (default: 1)

        Returns:
            List of PDF chunks as bytes
        """
        import PyPDF2
        import io

        pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
        total_pages = len(pdf_reader.pages)

        # If PDF is small, don't split
        if total_pages <= pages_per_chunk:
            return [pdf_bytes]

        print(f"[PDF Split] Total pages: {total_pages}, splitting into chunks of {pages_per_chunk} pages with {overlap} page overlap")

        chunks = []
        start = 0

        while start < total_pages:
            end = min(start + pages_per_chunk, total_pages)

            # Create a new PDF with pages [start, end)
            pdf_writer = PyPDF2.PdfWriter()
            for page_num in range(start, end):
                pdf_writer.add_page(pdf_reader.pages[page_num])

            # Write to bytes
            chunk_bytes = io.BytesIO()
            pdf_writer.write(chunk_bytes)
            chunk_bytes.seek(0)
            chunks.append(chunk_bytes.getvalue())

            print(f"[PDF Split] Chunk {len(chunks)}: pages {start+1}-{end}")

            # Move to next chunk with overlap
            start = end - overlap if end < total_pages else total_pages

        return chunks

    async def parse_document_with_pdf(self, pdf_bytes: bytes, filename: str) -> List[Dict[str, Any]]:
        """
        Parse PDF document using Gemini's native PDF understanding.
        Automatically splits large PDFs into overlapping chunks.
        Only works with Gemini provider.

        Args:
            pdf_bytes: PDF file content as bytes
            filename: Original filename for logging

        Returns:
            List of question dictionaries
        """
        if self.provider != "gemini":
            raise ValueError("PDF parsing is only supported with Gemini provider")

        # Split PDF into chunks
        pdf_chunks = self.split_pdf_pages(pdf_bytes, pages_per_chunk=4, overlap=1)

        print(f"[Gemini PDF] Processing {len(pdf_chunks)} chunk(s) for {filename}")

        all_questions = []
        # Process each chunk with fuzzy deduplication
        for chunk_idx, chunk_bytes in enumerate(pdf_chunks):
            print(f"[Gemini PDF] Processing chunk {chunk_idx + 1}/{len(pdf_chunks)}")

            try:
                questions = await self._parse_pdf_chunk(chunk_bytes, f"{filename}_chunk_{chunk_idx + 1}")
                print(f"[Gemini PDF] Chunk {chunk_idx + 1} extracted {len(questions)} questions")

                # Fuzzy deduplicate across chunks
                from dedup_utils import is_duplicate_question

                for q in questions:
                    if not is_duplicate_question(q, all_questions, threshold=0.85):
                        all_questions.append(q)
                    else:
                        print(f"[PDF Split] Skipped fuzzy duplicate from chunk {chunk_idx + 1}")

            except Exception as e:
                print(f"[Gemini PDF] Chunk {chunk_idx + 1} failed: {str(e)}")
                # Continue with other chunks
                continue

        print(f"[Gemini PDF] Total questions extracted: {len(all_questions)} (after deduplication)")

        return all_questions

    async def _parse_pdf_chunk(self, pdf_bytes: bytes, chunk_name: str) -> List[Dict[str, Any]]:
        """
        Parse a single PDF chunk.
        Internal method used by parse_document_with_pdf.
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯•é¢˜è§£æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æè¿™ä¸ª PDF æ–‡æ¡£ï¼Œæå–å…¶ä¸­çš„æ‰€æœ‰è¯•é¢˜ã€‚

**è¯†åˆ«è§„åˆ™**ï¼š
- PDF ä¸­å¯èƒ½åŒ…å«ä¸­æ–‡æˆ–è‹±æ–‡é¢˜ç›®ã€å›¾ç‰‡ã€è¡¨æ ¼ã€å…¬å¼
- é¢˜ç›®å¯èƒ½æœ‰å¤šç§æ ¼å¼ï¼Œè¯·çµæ´»è¯†åˆ«
- **é‡è¦**ï¼šåªæå–å®Œæ•´çš„é¢˜ç›®ï¼Œå¿½ç•¥ä»»ä½•ä¸å®Œæ•´çš„é¢˜ç›®ï¼ˆé¢˜ç›®è¢«æˆªæ–­ã€ç¼ºå°‘é€‰é¡¹ã€ç¼ºå°‘å…³é”®ä¿¡æ¯ç­‰ï¼‰
- å¦‚æœé¢˜ç›®çœ‹èµ·æ¥ä¸å®Œæ•´ï¼ˆæ¯”å¦‚å¼€å¤´æˆ–ç»“å°¾è¢«åˆ‡æ–­ï¼‰ï¼Œç›´æ¥è·³è¿‡è¯¥é¢˜ç›®
- é¢˜ç›®å†…å®¹å¦‚æœåŒ…å«ä»£ç æˆ–æ¢è¡Œï¼Œè¯·å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸º\\n
- å›¾ç‰‡ä¸­çš„æ–‡å­—ä¹Ÿè¦è¯†åˆ«å¹¶æå–

**é¢˜ç›®ç±»å‹è¯†åˆ«** (ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹4ç§ç±»å‹ä¹‹ä¸€)ï¼š
1. **single** - å•é€‰é¢˜ï¼šåªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆçš„é€‰æ‹©é¢˜
2. **multiple** - å¤šé€‰é¢˜ï¼šæœ‰å¤šä¸ªæ­£ç¡®ç­”æ¡ˆçš„é€‰æ‹©é¢˜ï¼ˆç­”æ¡ˆæ ¼å¼å¦‚ï¼šABã€ABCã€ACDç­‰ï¼‰
3. **judge** - åˆ¤æ–­é¢˜ï¼šå¯¹é”™/æ˜¯é/True Falseé¢˜ç›®
4. **short** - ç®€ç­”é¢˜ï¼šåŒ…æ‹¬é—®ç­”ã€è®¡ç®—ã€è¯æ˜ã€å¡«ç©ºã€ç¼–ç¨‹ç­‰æ‰€æœ‰éé€‰æ‹©é¢˜

**å¤šé€‰é¢˜è¯†åˆ«å…³é”®è¯**ï¼š
- æ˜ç¡®æ ‡æ³¨"å¤šé€‰"ã€"å¤šé¡¹é€‰æ‹©"ã€"Multiple Choice"
- é¢˜å¹²ä¸­åŒ…å«"å¯èƒ½"ã€"æ­£ç¡®çš„æœ‰"ã€"åŒ…æ‹¬"ç­‰
- ç­”æ¡ˆæ˜¯å¤šä¸ªå­—æ¯ç»„åˆï¼ˆå¦‚ï¼šABCã€BDã€ABCDï¼‰

**æ¯é“é¢˜ç›®æå–å­—æ®µ**ï¼š
1. **content**: å®Œæ•´çš„é¢˜ç›®æ–‡å­—ï¼ˆå»é™¤é¢˜å·ï¼Œæ¢è¡Œç”¨\\nè¡¨ç¤ºï¼‰
2. **type**: é¢˜ç›®ç±»å‹ï¼ˆsingle/multiple/judge/shortï¼‰
3. **options**: é€‰é¡¹æ•°ç»„ï¼ˆä»…é€‰æ‹©é¢˜ï¼Œæ ¼å¼: ["A. é€‰é¡¹1", "B. é€‰é¡¹2", ...]ï¼‰
4. **answer**: æ­£ç¡®ç­”æ¡ˆ
   - å•é€‰é¢˜: å•ä¸ªå­—æ¯ (å¦‚ "A"ã€"B")
   - å¤šé€‰é¢˜: å¤šä¸ªå­—æ¯æ— ç©ºæ ¼ (å¦‚ "AB"ã€"ABC"ã€"BD")
   - åˆ¤æ–­é¢˜: "å¯¹"/"é”™"ã€"æ­£ç¡®"/"é”™è¯¯"ã€"True"/"False"
   - ç®€ç­”é¢˜: å®Œæ•´ç­”æ¡ˆæ–‡æœ¬ï¼Œå¦‚æœæ²¡æœ‰ç­”æ¡ˆå¡« null
5. **analysis**: è§£æè¯´æ˜ï¼ˆå¦‚æœæœ‰ï¼‰

**JSON æ ¼å¼è¦æ±‚**ï¼š
1. **å¿…é¡»**è¿”å›ä¸€ä¸ªå®Œæ•´çš„ JSON æ•°ç»„ï¼ˆä»¥ [ å¼€å§‹ï¼Œä»¥ ] ç»“æŸï¼‰
2. **ä¸è¦**è¿”å› JSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
3. **ä¸è¦**åŒ…å« markdown ä»£ç å—æ ‡è®°ï¼ˆ```json æˆ– ```ï¼‰
4. **ä¸è¦**åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—
5. å­—ç¬¦ä¸²ä¸­çš„ç‰¹æ®Šå­—ç¬¦å¿…é¡»æ­£ç¡®è½¬ä¹‰ï¼ˆæ¢è¡Œç”¨ \\nï¼Œå¼•å·ç”¨ \\"ï¼Œåæ–œæ ç”¨ \\\\ï¼‰
6. ä¸è¦åœ¨å­—ç¬¦ä¸²å€¼ä¸­ä½¿ç”¨æœªè½¬ä¹‰çš„æ§åˆ¶å­—ç¬¦

**è¿”å›æ ¼å¼ç¤ºä¾‹**ï¼š
[
  {{
    "content": "ä¸‹åˆ—å…³äºPythonçš„æè¿°ï¼Œæ­£ç¡®çš„æ˜¯",
    "type": "single",
    "options": ["A. Pythonæ˜¯ç¼–è¯‘å‹è¯­è¨€", "B. Pythonæ”¯æŒé¢å‘å¯¹è±¡ç¼–ç¨‹", "C. Pythonä¸æ”¯æŒå‡½æ•°å¼ç¼–ç¨‹", "D. Pythonåªèƒ½ç”¨äºWebå¼€å‘"],
    "answer": "B",
    "analysis": "Pythonæ˜¯è§£é‡Šå‹è¯­è¨€ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼"
  }},
  {{
    "content": "ä»¥ä¸‹å“ªäº›æ˜¯Pythonçš„ä¼˜ç‚¹ï¼ˆå¤šé€‰ï¼‰",
    "type": "multiple",
    "options": ["A. è¯­æ³•ç®€æ´", "B. åº“ä¸°å¯Œ", "C. æ‰§è¡Œé€Ÿåº¦æœ€å¿«", "D. æ˜“äºå­¦ä¹ "],
    "answer": "ABD",
    "analysis": "Pythonä¼˜ç‚¹æ˜¯è¯­æ³•ç®€æ´ã€åº“ä¸°å¯Œã€æ˜“å­¦ï¼Œä½†æ‰§è¡Œé€Ÿåº¦ä¸æ˜¯æœ€å¿«çš„"
  }},
  {{
    "content": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
    "type": "judge",
    "options": [],
    "answer": "å¯¹",
    "analysis": null
  }}
]

**æœ€åæé†’**ï¼š
- è¯·ä»”ç»†æŸ¥çœ‹ PDF çš„æ¯ä¸€é¡µ
- ä»”ç»†è¯†åˆ«å¤šé€‰é¢˜ï¼ˆçœ‹é¢˜å¹²ã€çœ‹ç­”æ¡ˆæ ¼å¼ï¼‰
- å•é€‰å’Œå¤šé€‰å®¹æ˜“æ··æ·†ï¼Œè¯·ç‰¹åˆ«æ³¨æ„åŒºåˆ†
- å¦‚æœæ‰¾ä¸åˆ°æ˜ç¡®çš„é€‰é¡¹ï¼Œå¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­é¢˜ç›®ç±»å‹
- é¢˜ç›®å†…å®¹ä¸­çš„æ¢è¡Œè¯·ç”¨\\næˆ–ç©ºæ ¼æ›¿æ¢ï¼Œç¡®ä¿ JSON æ ¼å¼æ­£ç¡®
- **åªè¿”å›ä¸€ä¸ª JSON æ•°ç»„**ï¼Œä¸è¦åŒ…å«å…¶ä»–ä»»ä½•å†…å®¹"""

        try:
            print(f"[Gemini PDF] Processing chunk: {chunk_name}", flush=True)
            print(f"[Gemini PDF] Chunk size: {len(pdf_bytes)} bytes", flush=True)

            # Use Gemini's native PDF processing via REST API
            import base64

            # Encode PDF to base64
            pdf_base64 = base64.b64encode(pdf_bytes).decode('utf-8')
            print(f"[Gemini PDF] PDF encoded to base64: {len(pdf_base64)} chars", flush=True)

            # Build REST API request
            url = f"{self.gemini_base_url}/v1beta/models/{self.model}:generateContent"
            headers = {"Content-Type": "application/json"}
            params = {"key": self.gemini_api_key}
            payload = {
                "contents": [{
                    "parts": [
                        {"inline_data": {"mime_type": "application/pdf", "data": pdf_base64}},
                        {"text": prompt}
                    ]
                }]
            }

            print(f"[Gemini PDF] Calling Gemini REST API with model: {self.model}", flush=True)
            response = await self.client.post(url, headers=headers, params=params, json=payload)
            response.raise_for_status()
            print(f"[Gemini PDF] API call completed", flush=True)

            response_data = response.json()

            # Extract text from response
            result = response_data["candidates"][0]["content"]["parts"][0]["text"]
            print(f"[Gemini PDF] Response retrieved, checking content...", flush=True)

            # Log original response for debugging
            import sys
            print(f"[LLM Raw Response] Length: {len(result)} chars", flush=True)
            print(f"[LLM Raw Response] First 300 chars:\n{result[:300]}", flush=True)
            print(f"[LLM Raw Response] Last 200 chars:\n{result[-200:]}", flush=True)
            sys.stdout.flush()

            # Clean result and parse JSON (same as text method)
            result = result.strip()

            # Remove markdown code blocks
            if result.startswith("```json"):
                result = result[7:]
            elif result.startswith("```"):
                result = result[3:]

            if result.endswith("```"):
                result = result[:-3]

            result = result.strip()

            # Try to find JSON array if there's extra text
            if not result.startswith('['):
                start_idx = result.find('[')
                if start_idx != -1:
                    print(f"[JSON Cleanup] Found '[' at position {start_idx}, extracting array...", flush=True)
                    result = result[start_idx:]
                else:
                    print(f"[JSON Error] No '[' found in response!", flush=True)
                    raise Exception("LLM response does not contain a JSON array")

            if not result.endswith(']'):
                end_idx = result.rfind(']')
                if end_idx != -1:
                    print(f"[JSON Cleanup] Found last ']' at position {end_idx}", flush=True)
                    result = result[:end_idx + 1]

            result = result.strip()

            # Additional cleanup: fix common JSON issues
            # 1. Remove trailing commas before closing brackets
            import re
            result = re.sub(r',(\s*[}\]])', r'\1', result)

            # Log the cleaned result for debugging
            print(f"[LLM Cleaned JSON] Length: {len(result)} chars", flush=True)
            print(f"[LLM Cleaned JSON] First 300 chars:\n{result[:300]}", flush=True)

            try:
                questions = json.loads(result)
            except json.JSONDecodeError as je:
                print(f"[JSON Error] Failed to parse JSON at line {je.lineno}, column {je.colno}", flush=True)
                print(f"[JSON Error] Error: {je.msg}", flush=True)
                # Print context around the error
                lines = result.split('\n')
                if je.lineno <= len(lines):
                    start = max(0, je.lineno - 3)
                    end = min(len(lines), je.lineno + 2)
                    print(f"[JSON Error] Context (lines {start+1}-{end}):", flush=True)
                    for i in range(start, end):
                        marker = " >>> " if i == je.lineno - 1 else "     "
                        print(f"{marker}{i+1}: {lines[i]}", flush=True)
                raise Exception(f"Invalid JSON format from LLM: {je.msg} at line {je.lineno}")

            # Validate that we got a list
            if not isinstance(questions, list):
                raise Exception(f"Expected a list of questions, got {type(questions)}")

            if len(questions) == 0:
                # Provide more helpful error message
                print(f"[Gemini PDF] âš ï¸ Gemini returned empty array - PDF may not contain recognizable questions", flush=True)
                print(f"[Gemini PDF] ğŸ’¡ Trying to get Gemini's explanation...", flush=True)

                # Ask Gemini what it saw in the PDF using REST API
                explanation_payload = {
                    "contents": [{
                        "parts": [
                            {"inline_data": {"mime_type": "application/pdf", "data": pdf_base64}},
                            {"text": "Please describe what you see in this PDF document. What is the main content? Are there any questions, exercises, or test items? Respond in Chinese."}
                        ]
                    }]
                }

                explanation_response = await self.client.post(url, headers=headers, params=params, json=explanation_payload)
                explanation_response.raise_for_status()
                explanation_data = explanation_response.json()
                explanation = explanation_data["candidates"][0]["content"]["parts"][0]["text"]
                print(f"[Gemini PDF] ğŸ“„ Gemini sees: {explanation[:500]}...", flush=True)

                raise Exception(f"No questions found in PDF. Gemini's description: {explanation[:200]}...")

            # Validate and fix question types
            valid_types = {"single", "multiple", "judge", "short"}
            type_mapping = {
                "proof": "short",
                "essay": "short",
                "calculation": "short",
                "fill": "short",
                "å¡«ç©º": "short",
                "è¯æ˜": "short",
                "è®¡ç®—": "short",
                "é—®ç­”": "short",
                "å•é€‰": "single",
                "å¤šé€‰": "multiple",
                "åˆ¤æ–­": "judge",
                "ç®€ç­”": "short"
            }

            # Add content hash and validate types
            for q in questions:
                if "content" not in q:
                    print(f"[Warning] Question missing 'content' field: {q}", flush=True)
                    continue

                # Validate and fix question type
                q_type = q.get("type", "short")
                if isinstance(q_type, str):
                    q_type_lower = q_type.lower()
                    if q_type_lower not in valid_types:
                        # Try to map to valid type
                        if q_type_lower in type_mapping:
                            old_type = q_type
                            q["type"] = type_mapping[q_type_lower]
                            print(f"[Type Fix] Changed '{old_type}' to '{q['type']}' for question: {q['content'][:50]}...", flush=True)
                        else:
                            # Default to short answer
                            print(f"[Type Fix] Unknown type '{q_type}', defaulting to 'short' for question: {q['content'][:50]}...", flush=True)
                            q["type"] = "short"
                    else:
                        q["type"] = q_type_lower
                else:
                    q["type"] = "short"

                q["content_hash"] = calculate_content_hash(q["content"])

            print(f"[Gemini PDF] Successfully extracted {len(questions)} questions", flush=True)
            return questions

        except Exception as e:
            print(f"[Error] PDF parsing failed: {str(e)}", flush=True)
            raise Exception(f"Failed to parse PDF document: {str(e)}")

    async def grade_short_answer(
        self,
        question: str,
        correct_answer: str,
        user_answer: str
    ) -> Dict[str, Any]:
        """
        Grade a short answer question using AI.

        Returns:
        {
            "score": 0.0-1.0,
            "feedback": "Detailed feedback"
        }
        """
        prompt = f"""Grade the following short answer question.

Question: {question}

Standard Answer: {correct_answer}

Student Answer: {user_answer}

Provide a score from 0.0 to 1.0 (where 1.0 is perfect) and detailed feedback.

Return ONLY a JSON object:
{{
  "score": 0.85,
  "feedback": "Your detailed feedback here"
}}

Be fair but strict. Consider:
1. Correctness of key points
2. Completeness of answer
3. Clarity of expression

Return ONLY the JSON object, no markdown or explanations."""

        try:
            if self.provider == "anthropic":
                response = await self.client.messages.create(
                    model=self.model,
                    max_tokens=1024,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                result = response.content[0].text
            elif self.provider == "gemini":
                # Gemini uses REST API
                url = f"{self.gemini_base_url}/v1beta/models/{self.model}:generateContent"
                headers = {"Content-Type": "application/json"}
                params = {"key": self.gemini_api_key}
                payload = {
                    "contents": [{
                        "parts": [{"text": prompt}]
                    }]
                }

                response = await self.client.post(url, headers=headers, params=params, json=payload)
                response.raise_for_status()
                response_data = response.json()
                result = response_data["candidates"][0]["content"]["parts"][0]["text"]
            else:  # OpenAI or Qwen
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a fair and strict grader. Return only JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                )
                result = response.choices[0].message.content

            # Clean and parse JSON
            result = result.strip()
            if result.startswith("```json"):
                result = result[7:]
            if result.startswith("```"):
                result = result[3:]
            if result.endswith("```"):
                result = result[:-3]
            result = result.strip()

            grading = json.loads(result)
            return {
                "score": float(grading.get("score", 0.0)),
                "feedback": grading.get("feedback", "")
            }

        except Exception as e:
            print(f"Error grading answer: {e}")
            # Return default grading on error
            return {
                "score": 0.0,
                "feedback": "Unable to grade answer due to an error."
            }


# Singleton instance
llm_service = LLMService()
