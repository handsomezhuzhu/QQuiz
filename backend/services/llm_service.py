"""
LLM Service for AI-powered question parsing and grading
"""
import os
import json
from typing import List, Dict, Any, Optional
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
import httpx

from models import QuestionType
from utils import calculate_content_hash


class LLMService:
    """Service for interacting with various LLM providers"""

    def __init__(self, config: Optional[Dict[str, str]] = None):
        """
        Initialize LLM Service with optional configuration.
        If config is not provided, falls back to environment variables.

        Args:
            config: Dictionary with keys like 'ai_provider', 'openai_api_key', etc.
        """
        # Get provider from config or environment
        self.provider = (config or {}).get("ai_provider") or os.getenv("AI_PROVIDER", "openai")

        if self.provider == "openai":
            api_key = (config or {}).get("openai_api_key") or os.getenv("OPENAI_API_KEY")
            base_url = (config or {}).get("openai_base_url") or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
            self.model = (config or {}).get("openai_model") or os.getenv("OPENAI_MODEL", "gpt-4o-mini")

            if not api_key:
                raise ValueError("OpenAI API key not configured")

            self.client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=120.0,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’
                max_retries=3   # è‡ªåŠ¨é‡è¯• 3 æ¬¡
            )

            # Log configuration for debugging
            print(f"[LLM Config] Provider: OpenAI", flush=True)
            print(f"[LLM Config] Base URL: {base_url}", flush=True)
            print(f"[LLM Config] Model: {self.model}", flush=True)
            print(f"[LLM Config] API Key: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else 'xxxx'}", flush=True)

        elif self.provider == "anthropic":
            api_key = (config or {}).get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY")
            self.model = (config or {}).get("anthropic_model") or os.getenv("ANTHROPIC_MODEL", "claude-3-haiku-20240307")

            if not api_key:
                raise ValueError("Anthropic API key not configured")

            self.client = AsyncAnthropic(
                api_key=api_key
            )

        elif self.provider == "qwen":
            api_key = (config or {}).get("qwen_api_key") or os.getenv("QWEN_API_KEY")
            base_url = (config or {}).get("qwen_base_url") or os.getenv("QWEN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
            self.model = (config or {}).get("qwen_model") or os.getenv("QWEN_MODEL", "qwen-plus")

            if not api_key:
                raise ValueError("Qwen API key not configured")

            self.client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url,
                timeout=120.0,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 120 ç§’
                max_retries=3   # è‡ªåŠ¨é‡è¯• 3 æ¬¡
            )

        elif self.provider == "gemini":
            api_key = (config or {}).get("gemini_api_key") or os.getenv("GEMINI_API_KEY")
            base_url = (config or {}).get("gemini_base_url") or os.getenv("GEMINI_BASE_URL")
            self.model = (config or {}).get("gemini_model") or os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp")

            if not api_key:
                raise ValueError("Gemini API key not configured")

            # Store Gemini configuration for REST API calls
            self.gemini_api_key = api_key
            self.gemini_base_url = base_url or "https://generativelanguage.googleapis.com"

            # Create httpx client for REST API calls (instead of SDK)
            self.client = httpx.AsyncClient(
                timeout=120.0,
                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
            )

            # Log configuration for debugging
            print(f"[LLM Config] Provider: Gemini (REST API)", flush=True)
            print(f"[LLM Config] Model: {self.model}", flush=True)
            print(f"[LLM Config] Base URL: {self.gemini_base_url}", flush=True)
            print(f"[LLM Config] API Key: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else 'xxxx'}", flush=True)

        else:
            raise ValueError(f"Unsupported AI provider: {self.provider}")

    async def parse_document(self, content: str) -> List[Dict[str, Any]]:
        """
        Parse document content and extract questions.

        Returns a list of dictionaries with question data:
        [
            {
                "content": "Question text",
                "type": "single/multiple/judge/short",
                "options": ["A. Option1", "B. Option2", ...],  # For choice questions
                "answer": "Correct answer",
                "analysis": "Explanation"
            },
            ...
        ]
        """
        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯•é¢˜è§£æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä¸‹é¢çš„æ–‡æ¡£å†…å®¹ï¼Œæå–å…¶ä¸­çš„æ‰€æœ‰è¯•é¢˜ã€‚

è¯·æ³¨æ„ï¼š
- æ–‡æ¡£ä¸­å¯èƒ½åŒ…å«ä¸­æ–‡æˆ–è‹±æ–‡é¢˜ç›®
- é¢˜ç›®å¯èƒ½æœ‰å¤šç§æ ¼å¼ï¼Œè¯·çµæ´»è¯†åˆ«
- å³ä½¿æ ¼å¼ä¸æ ‡å‡†ï¼Œä¹Ÿè¯·å°½é‡æå–é¢˜ç›®å†…å®¹
- å¦‚æœæ–‡æ¡£åªæ˜¯æ™®é€šæ–‡ç« è€Œæ²¡æœ‰é¢˜ç›®ï¼Œè¯·è¿”å›ç©ºæ•°ç»„ []

å¯¹äºæ¯é“é¢˜ç›®ï¼Œè¯·è¯†åˆ«ï¼š
1. é¢˜ç›®å†…å®¹ (å®Œæ•´çš„é¢˜ç›®æ–‡å­—)
2. é¢˜ç›®ç±»å‹ï¼ˆ**åªèƒ½**ä½¿ç”¨ä»¥ä¸‹4ç§ç±»å‹ä¹‹ä¸€ï¼‰ï¼š
   - singleï¼šå•é€‰é¢˜
   - multipleï¼šå¤šé€‰é¢˜
   - judgeï¼šåˆ¤æ–­é¢˜
   - shortï¼šç®€ç­”é¢˜ï¼ˆåŒ…æ‹¬é—®ç­”é¢˜ã€è®¡ç®—é¢˜ã€è¯æ˜é¢˜ã€å¡«ç©ºé¢˜ç­‰æ‰€æœ‰éé€‰æ‹©é¢˜ï¼‰
3. é€‰é¡¹ (ä»…é’ˆå¯¹é€‰æ‹©é¢˜ï¼Œæ ¼å¼: ["A. é€‰é¡¹1", "B. é€‰é¡¹2", ...])
4. æ­£ç¡®ç­”æ¡ˆ (è¯·ä»”ç»†æŸ¥æ‰¾æ–‡æ¡£ä¸­çš„ç­”æ¡ˆã€‚å¦‚æœç¡®å®æ²¡æœ‰ç­”æ¡ˆï¼Œå¯ä»¥å¡« null)
5. è§£æ/è¯´æ˜ (å¦‚æœæœ‰çš„è¯)

**é‡è¦**ï¼šé¢˜ç›®ç±»å‹å¿…é¡»æ˜¯ singleã€multipleã€judgeã€short ä¹‹ä¸€ï¼Œä¸è¦ä½¿ç”¨å…¶ä»–ç±»å‹åç§°ï¼

è¿”å›æ ¼å¼ï¼šè¯·**åªè¿”å›** JSON æ•°ç»„ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—æˆ– markdown ä»£ç å—ï¼š
[
  {{
    "content": "é¢˜ç›®å†…å®¹",
    "type": "single",
    "options": ["A. é€‰é¡¹1", "B. é€‰é¡¹2", "C. é€‰é¡¹3", "D. é€‰é¡¹4"],
    "answer": "A",
    "analysis": "è§£æè¯´æ˜"
  }},
  ...
]

æ–‡æ¡£å†…å®¹ï¼š
---
{content}
---

é‡è¦æç¤ºï¼š
- ä»”ç»†é˜…è¯»æ–‡æ¡£å†…å®¹
- è¯†åˆ«æ‰€æœ‰çœ‹èµ·æ¥åƒè¯•é¢˜çš„å†…å®¹
- å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰é¢˜ç›®ï¼ˆæ¯”å¦‚åªæ˜¯æ™®é€šæ–‡ç« ï¼‰ï¼Œè¿”å› []
- **åªè¿”å› JSON æ•°ç»„**ï¼Œä¸è¦åŒ…å« ```json æˆ–å…¶ä»–æ ‡è®°"""

        try:
            if self.provider == "anthropic":
                response = await self.client.messages.create(
                    model=self.model,
                    max_tokens=4096,
                    messages=[
                        {"role": "user", "content": prompt.format(content=content)}
                    ]
                )
                result = response.content[0].text
            elif self.provider == "gemini":
                # Gemini uses REST API
                print(f"[Gemini Text] Calling Gemini REST API with model: {self.model}", flush=True)

                url = f"{self.gemini_base_url}/v1beta/models/{self.model}:generateContent"
                headers = {"Content-Type": "application/json"}
                params = {"key": self.gemini_api_key}
                payload = {
                    "contents": [{
                        "parts": [{"text": prompt.format(content=content)}]
                    }]
                }

                response = await self.client.post(url, headers=headers, params=params, json=payload)
                response.raise_for_status()
                response_data = response.json()

                # Extract text from response
                result = response_data["candidates"][0]["content"]["parts"][0]["text"]
                print(f"[Gemini Text] API call completed", flush=True)
            else:  # OpenAI or Qwen
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a professional question parser. Return only JSON."},
                        {"role": "user", "content": prompt.format(content=content)}
                    ],
                    temperature=0.3,
                )
                result = response.choices[0].message.content

            # Log original response for debugging
            import sys
            print(f"[LLM Raw Response] Length: {len(result)} chars", flush=True)
            print(f"[LLM Raw Response] First 300 chars:\n{result[:300]}", flush=True)
            print(f"[LLM Raw Response] Last 200 chars:\n{result[-200:]}", flush=True)
            sys.stdout.flush()

            # Clean result and parse JSON
            result = result.strip()

            # Remove markdown code blocks
            if result.startswith("```json"):
                result = result[7:]
            elif result.startswith("```"):
                result = result[3:]

            if result.endswith("```"):
                result = result[:-3]

            result = result.strip()

            # Try to find JSON array if there's extra text
            if not result.startswith('['):
                # Find the first '[' character
                start_idx = result.find('[')
                if start_idx != -1:
                    print(f"[JSON Cleanup] Found '[' at position {start_idx}, extracting array...")
                    result = result[start_idx:]
                else:
                    print(f"[JSON Error] No '[' found in response!")
                    raise Exception("LLM response does not contain a JSON array")

            if not result.endswith(']'):
                # Find the last ']' character
                end_idx = result.rfind(']')
                if end_idx != -1:
                    print(f"[JSON Cleanup] Found last ']' at position {end_idx}")
                    result = result[:end_idx + 1]

            result = result.strip()

            # Log the cleaned result for debugging
            print(f"[LLM Cleaned JSON] Length: {len(result)} chars")
            print(f"[LLM Cleaned JSON] First 300 chars:\n{result[:300]}")

            try:
                questions = json.loads(result)
            except json.JSONDecodeError as je:
                print(f"[JSON Error] Failed to parse JSON at line {je.lineno}, column {je.colno}")
                print(f"[JSON Error] Error: {je.msg}")

                # If error is about control characters, try to fix them
                if "control character" in je.msg.lower() or "invalid \\escape" in je.msg.lower():
                    print(f"[JSON Cleanup] Attempting to fix control characters...", flush=True)

                    # Fix unescaped control characters in JSON string values
                    import re

                    def fix_string_value(match):
                        """Fix control characters inside a JSON string value"""
                        string_content = match.group(1)
                        # Escape control characters
                        string_content = string_content.replace('\n', '\\n')
                        string_content = string_content.replace('\r', '\\r')
                        string_content = string_content.replace('\t', '\\t')
                        string_content = string_content.replace('\b', '\\b')
                        string_content = string_content.replace('\f', '\\f')
                        return f'"{string_content}"'

                    # Match string values in JSON
                    # Pattern matches: "..." (handles escaped quotes and backslashes)
                    # (?:[^"\\]|\\.)* means: either non-quote-non-backslash OR backslash-followed-by-anything, repeated
                    fixed_result = re.sub(r'"((?:[^"\\]|\\.)*)"', fix_string_value, result)

                    print(f"[JSON Cleanup] Retrying with fixed control characters...", flush=True)
                    try:
                        questions = json.loads(fixed_result)
                        print(f"[JSON Cleanup] âœ… Successfully parsed after fixing control characters!", flush=True)
                    except json.JSONDecodeError as je2:
                        print(f"[JSON Error] Still failed after fix: {je2.msg}", flush=True)
                        # Print context around the error
                        lines = result.split('\n')
                        if je.lineno <= len(lines):
                            start = max(0, je.lineno - 3)
                            end = min(len(lines), je.lineno + 2)
                            print(f"[JSON Error] Context (lines {start+1}-{end}):")
                            for i in range(start, end):
                                marker = " >>> " if i == je.lineno - 1 else "     "
                                print(f"{marker}{i+1}: {lines[i]}")
                        raise Exception(f"Invalid JSON format from LLM: {je.msg} at line {je.lineno}")
                else:
                    # Print context around the error
                    lines = result.split('\n')
                    if je.lineno <= len(lines):
                        start = max(0, je.lineno - 3)
                        end = min(len(lines), je.lineno + 2)
                        print(f"[JSON Error] Context (lines {start+1}-{end}):")
                        for i in range(start, end):
                            marker = " >>> " if i == je.lineno - 1 else "     "
                            print(f"{marker}{i+1}: {lines[i]}")
                    raise Exception(f"Invalid JSON format from LLM: {je.msg} at line {je.lineno}")

            # Validate that we got a list
            if not isinstance(questions, list):
                raise Exception(f"Expected a list of questions, got {type(questions)}")

            if len(questions) == 0:
                raise Exception("No questions found in the parsed result")

            # Validate and fix question types
            valid_types = {"single", "multiple", "judge", "short"}
            type_mapping = {
                "proof": "short",
                "essay": "short",
                "calculation": "short",
                "fill": "short",
                "å¡«ç©º": "short",
                "è¯æ˜": "short",
                "è®¡ç®—": "short",
                "é—®ç­”": "short",
                "å•é€‰": "single",
                "å¤šé€‰": "multiple",
                "åˆ¤æ–­": "judge",
                "ç®€ç­”": "short"
            }

            # Add content hash and validate types
            for q in questions:
                if "content" not in q:
                    print(f"[Warning] Question missing 'content' field: {q}")
                    continue

                # Validate and fix question type
                q_type = q.get("type", "short")
                if isinstance(q_type, str):
                    q_type_lower = q_type.lower()
                    if q_type_lower not in valid_types:
                        # Try to map to valid type
                        if q_type_lower in type_mapping:
                            old_type = q_type
                            q["type"] = type_mapping[q_type_lower]
                            print(f"[Type Fix] Changed '{old_type}' to '{q['type']}' for question: {q['content'][:50]}...", flush=True)
                        else:
                            # Default to short answer
                            print(f"[Type Fix] Unknown type '{q_type}', defaulting to 'short' for question: {q['content'][:50]}...", flush=True)
                            q["type"] = "short"
                    else:
                        q["type"] = q_type_lower
                else:
                    q["type"] = "short"

                q["content_hash"] = calculate_content_hash(q["content"])

            return questions

        except Exception as e:
            print(f"[Error] Document parsing failed: {str(e)}")
            raise Exception(f"Failed to parse document: {str(e)}")

    async def parse_document_with_pdf(self, pdf_bytes: bytes, filename: str) -> List[Dict[str, Any]]:
        """
        Parse PDF document using Gemini's native PDF understanding.
        Only works with Gemini provider.

        Args:
            pdf_bytes: PDF file content as bytes
            filename: Original filename for logging

        Returns:
            List of question dictionaries
        """
        if self.provider != "gemini":
            raise ValueError("PDF parsing is only supported with Gemini provider")

        prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯•é¢˜è§£æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æè¿™ä¸ª PDF æ–‡æ¡£ï¼Œæå–å…¶ä¸­çš„æ‰€æœ‰è¯•é¢˜ã€‚

è¯·æ³¨æ„ï¼š
- PDF ä¸­å¯èƒ½åŒ…å«ä¸­æ–‡æˆ–è‹±æ–‡é¢˜ç›®
- é¢˜ç›®å¯èƒ½æœ‰å¤šç§æ ¼å¼ï¼Œè¯·çµæ´»è¯†åˆ«
- å³ä½¿æ ¼å¼ä¸æ ‡å‡†ï¼Œä¹Ÿè¯·å°½é‡æå–é¢˜ç›®å†…å®¹
- é¢˜ç›®å†…å®¹å¦‚æœåŒ…å«ä»£ç æˆ–æ¢è¡Œï¼Œè¯·å°†æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼æˆ–\\n

å¯¹äºæ¯é“é¢˜ç›®ï¼Œè¯·è¯†åˆ«ï¼š
1. é¢˜ç›®å†…å®¹ (å®Œæ•´çš„é¢˜ç›®æ–‡å­—ï¼Œå¦‚æœæœ‰ä»£ç è¯·ä¿æŒåœ¨ä¸€è¡Œæˆ–ç”¨\\nè¡¨ç¤ºæ¢è¡Œ)
2. é¢˜ç›®ç±»å‹ï¼ˆ**åªèƒ½**ä½¿ç”¨ä»¥ä¸‹4ç§ç±»å‹ä¹‹ä¸€ï¼‰ï¼š
   - singleï¼šå•é€‰é¢˜
   - multipleï¼šå¤šé€‰é¢˜
   - judgeï¼šåˆ¤æ–­é¢˜
   - shortï¼šç®€ç­”é¢˜ï¼ˆåŒ…æ‹¬é—®ç­”é¢˜ã€è®¡ç®—é¢˜ã€è¯æ˜é¢˜ã€å¡«ç©ºé¢˜ç­‰æ‰€æœ‰éé€‰æ‹©é¢˜ï¼‰
3. é€‰é¡¹ (ä»…é’ˆå¯¹é€‰æ‹©é¢˜ï¼Œæ ¼å¼: ["A. é€‰é¡¹1", "B. é€‰é¡¹2", ...])
4. æ­£ç¡®ç­”æ¡ˆ (è¯·ä»”ç»†æŸ¥æ‰¾æ–‡æ¡£ä¸­çš„ç­”æ¡ˆã€‚å¦‚æœç¡®å®æ²¡æœ‰ç­”æ¡ˆï¼Œå¯ä»¥å¡« null)
5. è§£æ/è¯´æ˜ (å¦‚æœæœ‰çš„è¯)

**é‡è¦**ï¼šé¢˜ç›®ç±»å‹å¿…é¡»æ˜¯ singleã€multipleã€judgeã€short ä¹‹ä¸€ï¼Œä¸è¦ä½¿ç”¨å…¶ä»–ç±»å‹åç§°ï¼

è¿”å›æ ¼å¼è¦æ±‚ï¼š
1. **å¿…é¡»**è¿”å›ä¸€ä¸ªå®Œæ•´çš„ JSON æ•°ç»„ï¼ˆä»¥ [ å¼€å§‹ï¼Œä»¥ ] ç»“æŸï¼‰
2. **ä¸è¦**è¿”å› JSONL æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
3. **ä¸è¦**åŒ…å« markdown ä»£ç å—æ ‡è®°ï¼ˆ```json æˆ– ```ï¼‰
4. **ä¸è¦**åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—

æ­£ç¡®çš„æ ¼å¼ç¤ºä¾‹ï¼š
[
  {{
    "content": "é¢˜ç›®å†…å®¹",
    "type": "single",
    "options": ["A. é€‰é¡¹1", "B. é€‰é¡¹2", "C. é€‰é¡¹3", "D. é€‰é¡¹4"],
    "answer": "A",
    "analysis": "è§£æè¯´æ˜"
  }},
  {{
    "content": "ç¬¬äºŒé“é¢˜",
    "type": "judge",
    "options": [],
    "answer": "å¯¹",
    "analysis": null
  }}
]

é‡è¦æç¤ºï¼š
- è¯·ä»”ç»†æŸ¥çœ‹ PDF çš„æ¯ä¸€é¡µ
- è¯†åˆ«æ‰€æœ‰çœ‹èµ·æ¥åƒè¯•é¢˜çš„å†…å®¹
- å¦‚æœæ‰¾ä¸åˆ°æ˜ç¡®çš„é€‰é¡¹ï¼Œå¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­é¢˜ç›®ç±»å‹
- é¢˜ç›®å†…å®¹ä¸­çš„æ¢è¡Œè¯·ç”¨\\næˆ–ç©ºæ ¼æ›¿æ¢ï¼Œç¡®ä¿ JSON æ ¼å¼æ­£ç¡®
- **åªè¿”å›ä¸€ä¸ª JSON æ•°ç»„**ï¼Œä¸è¦åŒ…å«å…¶ä»–ä»»ä½•å†…å®¹"""

        try:
            print(f"[Gemini PDF] Processing PDF: {filename}", flush=True)
            print(f"[Gemini PDF] File size: {len(pdf_bytes)} bytes", flush=True)

            # Use Gemini's native PDF processing via REST API
            import base64

            # Encode PDF to base64
            pdf_base64 = base64.b64encode(pdf_bytes).decode('utf-8')
            print(f"[Gemini PDF] PDF encoded to base64: {len(pdf_base64)} chars", flush=True)

            # Build REST API request
            url = f"{self.gemini_base_url}/v1beta/models/{self.model}:generateContent"
            headers = {"Content-Type": "application/json"}
            params = {"key": self.gemini_api_key}
            payload = {
                "contents": [{
                    "parts": [
                        {"inline_data": {"mime_type": "application/pdf", "data": pdf_base64}},
                        {"text": prompt}
                    ]
                }]
            }

            print(f"[Gemini PDF] Calling Gemini REST API with model: {self.model}", flush=True)
            response = await self.client.post(url, headers=headers, params=params, json=payload)
            response.raise_for_status()
            print(f"[Gemini PDF] API call completed", flush=True)

            response_data = response.json()

            # Extract text from response
            result = response_data["candidates"][0]["content"]["parts"][0]["text"]
            print(f"[Gemini PDF] Response retrieved, checking content...", flush=True)

            # Log original response for debugging
            import sys
            print(f"[LLM Raw Response] Length: {len(result)} chars", flush=True)
            print(f"[LLM Raw Response] First 300 chars:\n{result[:300]}", flush=True)
            print(f"[LLM Raw Response] Last 200 chars:\n{result[-200:]}", flush=True)
            sys.stdout.flush()

            # Clean result and parse JSON (same as text method)
            result = result.strip()

            # Remove markdown code blocks
            if result.startswith("```json"):
                result = result[7:]
            elif result.startswith("```"):
                result = result[3:]

            if result.endswith("```"):
                result = result[:-3]

            result = result.strip()

            # Try to find JSON array if there's extra text
            if not result.startswith('['):
                start_idx = result.find('[')
                if start_idx != -1:
                    print(f"[JSON Cleanup] Found '[' at position {start_idx}, extracting array...", flush=True)
                    result = result[start_idx:]
                else:
                    print(f"[JSON Error] No '[' found in response!", flush=True)
                    raise Exception("LLM response does not contain a JSON array")

            if not result.endswith(']'):
                end_idx = result.rfind(']')
                if end_idx != -1:
                    print(f"[JSON Cleanup] Found last ']' at position {end_idx}", flush=True)
                    result = result[:end_idx + 1]

            result = result.strip()

            # Log the cleaned result for debugging
            print(f"[LLM Cleaned JSON] Length: {len(result)} chars", flush=True)
            print(f"[LLM Cleaned JSON] First 300 chars:\n{result[:300]}", flush=True)

            try:
                questions = json.loads(result)
            except json.JSONDecodeError as je:
                print(f"[JSON Error] Failed to parse JSON at line {je.lineno}, column {je.colno}", flush=True)
                print(f"[JSON Error] Error: {je.msg}", flush=True)
                # Print context around the error
                lines = result.split('\n')
                if je.lineno <= len(lines):
                    start = max(0, je.lineno - 3)
                    end = min(len(lines), je.lineno + 2)
                    print(f"[JSON Error] Context (lines {start+1}-{end}):", flush=True)
                    for i in range(start, end):
                        marker = " >>> " if i == je.lineno - 1 else "     "
                        print(f"{marker}{i+1}: {lines[i]}", flush=True)
                raise Exception(f"Invalid JSON format from LLM: {je.msg} at line {je.lineno}")

            # Validate that we got a list
            if not isinstance(questions, list):
                raise Exception(f"Expected a list of questions, got {type(questions)}")

            if len(questions) == 0:
                # Provide more helpful error message
                print(f"[Gemini PDF] âš ï¸ Gemini returned empty array - PDF may not contain recognizable questions", flush=True)
                print(f"[Gemini PDF] ğŸ’¡ Trying to get Gemini's explanation...", flush=True)

                # Ask Gemini what it saw in the PDF using REST API
                explanation_payload = {
                    "contents": [{
                        "parts": [
                            {"inline_data": {"mime_type": "application/pdf", "data": pdf_base64}},
                            {"text": "Please describe what you see in this PDF document. What is the main content? Are there any questions, exercises, or test items? Respond in Chinese."}
                        ]
                    }]
                }

                explanation_response = await self.client.post(url, headers=headers, params=params, json=explanation_payload)
                explanation_response.raise_for_status()
                explanation_data = explanation_response.json()
                explanation = explanation_data["candidates"][0]["content"]["parts"][0]["text"]
                print(f"[Gemini PDF] ğŸ“„ Gemini sees: {explanation[:500]}...", flush=True)

                raise Exception(f"No questions found in PDF. Gemini's description: {explanation[:200]}...")

            # Validate and fix question types
            valid_types = {"single", "multiple", "judge", "short"}
            type_mapping = {
                "proof": "short",
                "essay": "short",
                "calculation": "short",
                "fill": "short",
                "å¡«ç©º": "short",
                "è¯æ˜": "short",
                "è®¡ç®—": "short",
                "é—®ç­”": "short",
                "å•é€‰": "single",
                "å¤šé€‰": "multiple",
                "åˆ¤æ–­": "judge",
                "ç®€ç­”": "short"
            }

            # Add content hash and validate types
            for q in questions:
                if "content" not in q:
                    print(f"[Warning] Question missing 'content' field: {q}", flush=True)
                    continue

                # Validate and fix question type
                q_type = q.get("type", "short")
                if isinstance(q_type, str):
                    q_type_lower = q_type.lower()
                    if q_type_lower not in valid_types:
                        # Try to map to valid type
                        if q_type_lower in type_mapping:
                            old_type = q_type
                            q["type"] = type_mapping[q_type_lower]
                            print(f"[Type Fix] Changed '{old_type}' to '{q['type']}' for question: {q['content'][:50]}...", flush=True)
                        else:
                            # Default to short answer
                            print(f"[Type Fix] Unknown type '{q_type}', defaulting to 'short' for question: {q['content'][:50]}...", flush=True)
                            q["type"] = "short"
                    else:
                        q["type"] = q_type_lower
                else:
                    q["type"] = "short"

                q["content_hash"] = calculate_content_hash(q["content"])

            print(f"[Gemini PDF] Successfully extracted {len(questions)} questions", flush=True)
            return questions

        except Exception as e:
            print(f"[Error] PDF parsing failed: {str(e)}", flush=True)
            raise Exception(f"Failed to parse PDF document: {str(e)}")

    async def grade_short_answer(
        self,
        question: str,
        correct_answer: str,
        user_answer: str
    ) -> Dict[str, Any]:
        """
        Grade a short answer question using AI.

        Returns:
        {
            "score": 0.0-1.0,
            "feedback": "Detailed feedback"
        }
        """
        prompt = f"""Grade the following short answer question.

Question: {question}

Standard Answer: {correct_answer}

Student Answer: {user_answer}

Provide a score from 0.0 to 1.0 (where 1.0 is perfect) and detailed feedback.

Return ONLY a JSON object:
{{
  "score": 0.85,
  "feedback": "Your detailed feedback here"
}}

Be fair but strict. Consider:
1. Correctness of key points
2. Completeness of answer
3. Clarity of expression

Return ONLY the JSON object, no markdown or explanations."""

        try:
            if self.provider == "anthropic":
                response = await self.client.messages.create(
                    model=self.model,
                    max_tokens=1024,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                result = response.content[0].text
            elif self.provider == "gemini":
                # Gemini uses REST API
                url = f"{self.gemini_base_url}/v1beta/models/{self.model}:generateContent"
                headers = {"Content-Type": "application/json"}
                params = {"key": self.gemini_api_key}
                payload = {
                    "contents": [{
                        "parts": [{"text": prompt}]
                    }]
                }

                response = await self.client.post(url, headers=headers, params=params, json=payload)
                response.raise_for_status()
                response_data = response.json()
                result = response_data["candidates"][0]["content"]["parts"][0]["text"]
            else:  # OpenAI or Qwen
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a fair and strict grader. Return only JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.5,
                )
                result = response.choices[0].message.content

            # Clean and parse JSON
            result = result.strip()
            if result.startswith("```json"):
                result = result[7:]
            if result.startswith("```"):
                result = result[3:]
            if result.endswith("```"):
                result = result[:-3]
            result = result.strip()

            grading = json.loads(result)
            return {
                "score": float(grading.get("score", 0.0)),
                "feedback": grading.get("feedback", "")
            }

        except Exception as e:
            print(f"Error grading answer: {e}")
            # Return default grading on error
            return {
                "score": 0.0,
                "feedback": "Unable to grade answer due to an error."
            }


# Singleton instance
llm_service = LLMService()
